I"~R<p>이번 글에서는 <strong>Pytorch</strong>와 <strong>MovieLens</strong> 데이터셋을 이용하여, 협업필터링을 구현하겠습니다. 협업 필터링의 여러 기법 중에서 <strong>Matrix Factorization</strong>을 사용하겠습니다. 마지막으로 <code class="language-plaintext highlighter-rouge">Neural Collaborative Filtering</code> 논문에서 제안한 Generalized Matrix Factorization 모델에 대해서 알아보고, 기존 알고리즘과의 성능 비교 실험을 해보겠습니다.</p>

<h2 id="1-matrix-factorization-소개">1. Matrix Factorization 소개</h2>

<p>유저 벡터($p_u$)와 아이템 벡터($p_i$)가 주어졌을 때, 유저와 아이템의 <code class="language-plaintext highlighter-rouge">상호작용(interaction)</code>을 다음과 같이 내적으로 정의합니다.</p>

<blockquote>
  <p><em>Matrix Factorization Model</em></p>

  <p>$y_{ui} = p_u \cdot p_i$</p>
</blockquote>

<p>이를 바탕으로 모델을 정의하면 다음과 같습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MF</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">user_num</span><span class="p">,</span> <span class="n">item_num</span><span class="p">,</span> <span class="n">factor_num</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MF</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">factor_num</span> <span class="o">=</span> <span class="n">factor_num</span>

        <span class="c1"># 임베딩 저장공간 확보; (num_embeddings, embedding_dim)
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">embed_user</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">user_num</span><span class="p">,</span> <span class="n">factor_num</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">embed_item</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">item_num</span><span class="p">,</span> <span class="n">factor_num</span><span class="p">)</span>
        <span class="n">predict_size</span> <span class="o">=</span> <span class="n">factor_num</span>
        <span class="c1"># 상수 Tensor 생성
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">predict_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">predict_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="n">cuda</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_init_weight_</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_init_weight_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># weight 초기화
</span>        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">embed_user</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">embed_item</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

        <span class="c1"># bias 초기화
</span>        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">)</span> <span class="ow">and</span> <span class="n">m</span><span class="p">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">m</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">zero_</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">user</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
        <span class="n">embed_user</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">embed_user</span><span class="p">(</span><span class="n">user</span><span class="p">)</span>
        <span class="n">embed_item</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">embed_item</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
        <span class="c1"># Tensor의 원소별 곱셈
</span>        <span class="n">output_GMF</span> <span class="o">=</span> <span class="n">embed_user</span> <span class="o">*</span> <span class="n">embed_item</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">output_GMF</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">predict_layer</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">prediction</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="generalized-matrix-factorization">Generalized Matrix Factorization</h2>

<p><strong>GMF</strong>는 이러한 상호작용을 일반화하여 모델의 성능을 향상시키기 위한 모델입니다. 이를 위해 위식 에서 <strong>1. 내적($\cdot$)을 일반화하고</strong>, <strong>2. 활성함수(activation function)를 추가합니다.</strong></p>

<blockquote>
  <p><em>Generalized Matrix Factorization (GMF)</em></p>

  <p>$y_{ui} = a_{out}(h^T(p_u \odot p_i))$</p>
</blockquote>

<p>이 모델을 구현한 코드는 다음과 같습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GMF</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">user_num</span><span class="p">,</span> <span class="n">item_num</span><span class="p">,</span> <span class="n">factor_num</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GMF</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>

        <span class="c1"># 임베딩 저장공간 확보; num_embeddings, embedding_dim
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">embed_user_GMF</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">user_num</span><span class="p">,</span> <span class="n">factor_num</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">embed_item_GMF</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">item_num</span><span class="p">,</span> <span class="n">factor_num</span><span class="p">)</span>
        <span class="n">predict_size</span> <span class="o">=</span> <span class="n">factor_num</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">predict_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">predict_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_init_weight_</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_init_weight_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># weight 초기화
</span>        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">embed_user_GMF</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">embed_item_GMF</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">predict_layer</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s">"sigmoid"</span><span class="p">)</span>

        <span class="c1"># bias 초기화
</span>        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">)</span> <span class="ow">and</span> <span class="n">m</span><span class="p">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">m</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">zero_</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">user</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
        <span class="n">embed_user_GMF</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">embed_user_GMF</span><span class="p">(</span><span class="n">user</span><span class="p">)</span>
        <span class="n">embed_item_GMF</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">embed_item_GMF</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
        <span class="n">output_GMF</span> <span class="o">=</span> <span class="n">embed_user_GMF</span> <span class="o">*</span> <span class="n">embed_item_GMF</span>
        <span class="n">concat</span> <span class="o">=</span> <span class="n">output_GMF</span>

        <span class="n">prediction</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">predict_layer</span><span class="p">(</span><span class="n">concat</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">prediction</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>손실함수는 <code class="language-plaintext highlighter-rouge">binary cross entropy</code>를, <code class="language-plaintext highlighter-rouge">optimizer</code>는 <code class="language-plaintext highlighter-rouge">Adam</code>을 사용하였습니다. 
이 외의 설정은 다음과 같습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">args</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"batch_size"</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span>
    <span class="s">"epochs"</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
    <span class="s">"factor_num"</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
    <span class="s">"gpu"</span><span class="p">:</span> <span class="s">"0"</span><span class="p">,</span>
    <span class="s">"lr"</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span>
    <span class="s">"num_layers"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="s">"num_ng"</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="s">"out"</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
    <span class="s">"test_num_ng"</span><span class="p">:</span> <span class="mi">99</span><span class="p">,</span>
    <span class="s">"top_k"</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p>전체코드는 다음 링크에서 확인하실 수 있습니다.</p>

<ul>
  <li>Matrix Factorization <a href="https://github.com/doheelab/NCF/blob/master/MF.py">(github)</a></li>
  <li>Generalized Matrix Factorization <a href="https://github.com/doheelab/NCF/blob/master/GMF.py">(github)</a></li>
</ul>

<h2 id="실험결과">실험결과</h2>

<p>20 epoch 동안 학습을 한 결과, <code class="language-plaintext highlighter-rouge">MF</code>의 최고 점수는 <code class="language-plaintext highlighter-rouge">HR = 0.704</code>, <code class="language-plaintext highlighter-rouge">NDCG = 0.422</code>이고, <code class="language-plaintext highlighter-rouge">GMF</code>의 치고 점수는 <code class="language-plaintext highlighter-rouge">HR = 0.706, NDCG = 0.423</code>이 나왔습니다. <code class="language-plaintext highlighter-rouge">GMF</code>의 스코어가 약간 더 높지만, 크게 의미있는 차이는 아닌 것 같습니다. 다만 <code class="language-plaintext highlighter-rouge">GMF</code>는 딥러닝 모델이므로 더 큰 데이터에 대해서 테스트를 하면 의미있는 차이가 나올 수도 있습니다.</p>

<blockquote>
  <p>실험결과 (MF)</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The time elapse of epoch 000 is: 00: 03: 05
HR: 0.514       NDCG: 0.290
The time elapse of epoch 001 is: 00: 03: 01
HR: 0.600       NDCG: 0.342
The time elapse of epoch 002 is: 00: 02: 59
HR: 0.644       NDCG: 0.372
The time elapse of epoch 003 is: 00: 02: 58
HR: 0.669       NDCG: 0.392
The time elapse of epoch 004 is: 00: 03: 05
HR: 0.680       NDCG: 0.401
The time elapse of epoch 005 is: 00: 03: 01
HR: 0.688       NDCG: 0.408
The time elapse of epoch 006 is: 00: 02: 58
HR: 0.694       NDCG: 0.415
The time elapse of epoch 007 is: 00: 03: 00
HR: 0.699       NDCG: 0.418
The time elapse of epoch 008 is: 00: 03: 01
HR: 0.702       NDCG: 0.418
The time elapse of epoch 009 is: 00: 03: 08
HR: 0.698       NDCG: 0.420
The time elapse of epoch 010 is: 00: 03: 04
HR: 0.704       NDCG: 0.422
The time elapse of epoch 011 is: 00: 03: 04
HR: 0.701       NDCG: 0.422
The time elapse of epoch 012 is: 00: 02: 56
HR: 0.704       NDCG: 0.423
The time elapse of epoch 013 is: 00: 02: 52
HR: 0.702       NDCG: 0.421
The time elapse of epoch 014 is: 00: 02: 53
HR: 0.703       NDCG: 0.423
The time elapse of epoch 015 is: 00: 02: 53
HR: 0.701       NDCG: 0.424
The time elapse of epoch 016 is: 00: 02: 52
HR: 0.699       NDCG: 0.419
The time elapse of epoch 017 is: 00: 02: 46
HR: 0.699       NDCG: 0.419
The time elapse of epoch 018 is: 00: 02: 45
HR: 0.697       NDCG: 0.420
The time elapse of epoch 019 is: 00: 02: 46
HR: 0.698       NDCG: 0.421
End. Best epoch 010: HR = 0.704, NDCG = 0.422
</code></pre></div>  </div>
</blockquote>

<blockquote>
  <p>실험결과 (GMF)</p>
</blockquote>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The time elapse of epoch 000 is: 00: 03: 11
HR: 0.572       NDCG: 0.320
The time elapse of epoch 001 is: 00: 03: 07
HR: 0.625       NDCG: 0.360
The time elapse of epoch 002 is: 00: 03: 08
HR: 0.651       NDCG: 0.382
The time elapse of epoch 003 is: 00: 03: 06
HR: 0.665       NDCG: 0.393
The time elapse of epoch 004 is: 00: 03: 03
HR: 0.681       NDCG: 0.404
The time elapse of epoch 005 is: 00: 02: 50
HR: 0.695       NDCG: 0.411
The time elapse of epoch 006 is: 00: 02: 55
HR: 0.699       NDCG: 0.413
The time elapse of epoch 007 is: 00: 02: 55
HR: 0.701       NDCG: 0.418
The time elapse of epoch 008 is: 00: 03: 09
HR: 0.705       NDCG: 0.420
The time elapse of epoch 009 is: 00: 03: 07
HR: 0.703       NDCG: 0.419
The time elapse of epoch 010 is: 00: 03: 03
HR: 0.700       NDCG: 0.420
The time elapse of epoch 011 is: 00: 03: 04
HR: 0.701       NDCG: 0.421
The time elapse of epoch 012 is: 00: 03: 10
HR: 0.702       NDCG: 0.421
The time elapse of epoch 013 is: 00: 03: 06
HR: 0.705       NDCG: 0.423
The time elapse of epoch 014 is: 00: 03: 04
HR: 0.701       NDCG: 0.425
The time elapse of epoch 015 is: 00: 03: 03
HR: 0.703       NDCG: 0.425
The time elapse of epoch 016 is: 00: 03: 07
HR: 0.704       NDCG: 0.425
The time elapse of epoch 017 is: 00: 03: 11
HR: 0.706       NDCG: 0.423
The time elapse of epoch 018 is: 00: 03: 09
HR: 0.702       NDCG: 0.424
The time elapse of epoch 019 is: 00: 03: 10
HR: 0.703       NDCG: 0.426
End. Best epoch 017: HR = 0.706, NDCG = 0.423
</code></pre></div></div>

<h2 id="참고자료">참고자료</h2>

<p>[1] <a href="https://github.com/doheelab/NCF">실습 코드 링크</a></p>

<p>[2] <a href="https://arxiv.org/abs/1708.05031">Neural Collaborative Filtering</a></p>

<p>[3] <a href="https://github.com/guoyang9/NCF">A pytorch GPU implementation of He et al.</a></p>

<p>[4] <a href="https://files.grouplens.org/datasets/movielens/ml-1m-README.txt">movielens dataset</a></p>

:ET