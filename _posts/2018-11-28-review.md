---
title: "[Review] Feature Pyramid Networks for Object Detection"
date: 2019-11-19 09:00:00 -0400
categories: machine-learning
---

## [(paper link)][Lin]


## List of contents
1. Introduction
2. Feature Pyramids structure
3. Network details
4. Applications
5. Experiments
6. Conclusion


## 1. Introduction
In this paper, authors introduce the multi-scale, pyramidal hierarchy of deep convolutional networks to construct **feature pyramids** with marginal extra cost. Using FPN in a basic Faster R-CNN system, the method achieves the best single-model performance on the COCO detection benchmark surpassing all existing models in 2017.


## 2. Feature Pyramids structure

<div style="text-align:center"><img src="http://playagricola.com/Kaggle/tran.jpg" /></div>

The principle advantage of featurizing each level of an image pyramid is that it produces a multi-scale feature representation in which all levels are semantically strong, including the high-resolution levels.
  
Let's look at some examples of feature pyramids.

  
> <div style="text-align:center"><img src="http://playagricola.com/Kaggle/cred.jpg" /></div>

In a featurized image network, we use an pyramid of images. Since we use multiple images of different scales to output a prediction, the inference time increases multiple times. This causes a long computational time and makes the model  impractical for real applications.


> <div style="text-align:center"><img src="http://playagricola.com/Kaggle/cred.jpg" /></div>

The single feature map uses the single scale features for faster detection. This is the basic structure of recent CNN models and has a short computational time. However, it cannot achieve the most accurate results because it lose much of spatial and semantic information in the lower level features.


> <div style="text-align:center"><img src="http://playagricola.com/Kaggle/cred.jpg" /></div>

An alternative is to reuse the pyramidal feature hierarchy computed by a CNN as if it were a featurized image pyramid. However, the prediction for each scale is does not affect each other.


> <div style="text-align:center"><img src="http://playagricola.com/Kaggle/cred.jpg" /></div>

Feature Pyramid Network (FPN) is probably better, and it performs fast and accurately. This model leverage the pyramidal shape of a ConvNet’s feature hierarchy while creating a feature pyramid that has **strong semantics at all scales**. It combines low-resolution, semantically strong features with high-resolution, semantically weak features via a top-down pathway and lateral connections.

## Data Explanation

In the original dataframe, there are 392 features including both numeric features and categrical features. 

![Selection_004](https://user-images.githubusercontent.com/57972646/69213947-f28d9e00-0ba8-11ea-8347-61bfd27f4f3c.png)

## Feature Information

### Transaction Table
* TransactionDT: timedelta from a given reference datetime (not an actual timestamp)
* TransactionAMT: transaction payment amount in USD
* ProductCD: product code, the product for each transaction
* card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.
* addr: address
* dist: distance
* P_ and (R__) emaildomain: purchaser and recipient email domain
* C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.
* D1-D15: timedelta, such as days between previous transaction, etc.
* M1-M9: match, such as names on card and address, etc.
* Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.

Categorical Features:
* ProductCD
* card1 - card6
* addr1, addr2
* Pemaildomain Remaildomain
* M1 - M9

### Identity Table

Variables in this table are identity information – network connection information (IP, ISP, Proxy, etc) and digital signature (UA/browser/os/version, etc) associated with transactions. 
They're collected by Vesta’s fraud protection system and digital security partners.
(The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement)

Categorical Features:
* DeviceType <br/>
* DeviceInfo <br/>
* id12 - id38 

### Read data

```python
import numpy as np, pandas as pd, os, gc
from sklearn.model_selection import GroupKFold
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns

# LOAD TRAIN
X_train = pd.read_csv('../input/ieee-fraud-detection/train_transaction.csv',index_col='TransactionID', nrows=10000)
train_id = pd.read_csv('../input/ieee-fraud-detection/train_identity.csv',index_col='TransactionID', nrows=10000)
X_train = X_train.merge(train_id, how='left', left_index=True, right_index=True)
# LOAD TEST
X_test = pd.read_csv('../input/ieee-fraud-detection/test_transaction.csv',index_col='TransactionID', nrows=10000)
test_id = pd.read_csv('../input/ieee-fraud-detection/test_identity.csv',index_col='TransactionID', nrows=10000)
X_test = X_test.merge(test_id, how='left', left_index=True, right_index=True)
# TARGET
y_train = X_train['isFraud'].copy()
del train_id, test_id, X_train['isFraud']; 
x = gc.collect()
```

```python
# PRINT STATUS
>>>print('Train shape',X_train.shape,'test shape',X_test.shape)
Train shape (10000, 432) test shape (10000, 432)
```
## Basic Feature Engineering

We use pandas's factorize function to convert categorical variables into numeric variables.

### Pandas.factorize
pandas.factorize(values, sort=False, order=None, na_sentinel=-1, size_hint=None) <br/>
Encode the object as an enumerated type or categorical variable.

```python
>>> labels, uniques = pd.factorize(['b', 'b', 'a', 'c', 'b'])
>>> labels
array([0, 0, 1, 2, 0])
>>> uniques
array(['b', 'a', 'c'], dtype=object)
```

The next things to do are:

1. (factorize) Convert categorical variables using pandas' factorize function.
2. (memory reduce) If the max value is 32000 or higher, the data type is converted to 'int32' type, otherwise it is converted to 'int32'.
3. (positive) Change all numeric values to zero or above. 
4. (NAN to -1) Convert all NAN values to -1.

```python
# LABEL ENCODE AND MEMORY REDUCE
for i,f in enumerate(X_train.columns):
    # FACTORIZE CATEGORICAL VARIABLES
    if (np.str(X_train[f].dtype)=='category')|(X_train[f].dtype=='object'): 
        df_comb = pd.concat([X_train[f],X_test[f]],axis=0)
        df_comb,_ = df_comb.factorize(sort=True)        # (factorize)
        if df_comb.max()>32000: print(f,'needs int32')  # (memory reduce) 
        X_train[f] = df_comb[:len(X_train)].astype('int16')
        X_test[f] = df_comb[len(X_train):].astype('int16')
        
    # SHIFT ALL NUMERICS POSITIVE. SET NAN to -1
    elif f not in ['TransactionAmt','TransactionDT']:
        mn = np.min((X_train[f].min(),X_test[f].min()))
        X_train[f] -= np.float32(mn)                   # (positive)
        X_test[f] -= np.float32(mn)                    # (NAN to -1)
        X_train[f].fillna(-1,inplace=True)
        X_test[f].fillna(-1,inplace=True)
```

Since this is time series data, we use the first 75% as the train set and the latter 25% as the validation set.
```python
# CHRIS - TRAIN 75% PREDICT 25%
idxT = X_train.index[:3*len(X_train)//4]
idxV = X_train.index[3*len(X_train)//4:]
```

We will now test the performance of the original version of the XGBoost model.


```python
import xgboost as xgb
print("XGBoost version:", xgb.__version__)

clf = xgb.XGBClassifier( 
    n_estimators=2000,
    max_depth=12, 
    learning_rate=0.02, 
    subsample=0.8,
    colsample_bytree=0.4, 
    missing=-1, 
    eval_metric='auc',
    # USE CPU
    nthread=4,
    tree_method='hist' 
    # USE GPU
    #tree_method='gpu_hist' 
)
h = clf.fit(X_train.loc[idxT], y_train[idxT], 
    eval_set=[(X_train.loc[idxV],y_train[idxV])],
    verbose=50, early_stopping_rounds=100)

feature_imp = pd.DataFrame(sorted(zip(clf.feature_importances_,cols)), columns=['Value','Feature'])
plt.figure(figsize=(20, 10))
sns.barplot(x="Value", y="Feature", data=feature_imp.sort_values(by="Value", ascending=False).iloc[:50])
plt.title('XGB95 Most Important Features')
plt.tight_layout()
plt.show()
del clf, h; x=gc.collect()
```
The result is as follows:
```
XGBoost version: 0.90
[0]	validation_0-auc:0.677229
Will train until validation_0-auc hasn't improved in 100 rounds.
[50]	validation_0-auc:0.831485
[100]	validation_0-auc:0.841948
[150]	validation_0-auc:0.857862
[200]	validation_0-auc:0.860735
[250]	validation_0-auc:0.868282
[300]	validation_0-auc:0.867505
Stopping. Best iteration:
[245]	validation_0-auc:0.86896
```

## Advanced Feature Engineering using the Magic Features
Let's take a look at how to use the magic feature to improve the performance of the original XGBoost model. <br/>
This requires two other operations. <br/> 
>COMBINE FEATURES

Concatenate two string type features to create a new feature. Ex) Hyundai Card + Suwon = Hyundai Card_Suwon <br/>
>GROUP AGGREGATION MEAN AND STD

Based on one feature, group the items belonging to the same class to find mean and std and add each new feature.

```python
# COMBINE FEATURES
def encode_CB(col1,col2,df1=X_train,df2=X_test):
    nm = col1+'_'+col2
    df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)
    df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str) 
    encode_LE(nm,verbose=False)
    print(nm,', ',end='')

# GROUP AGGREGATION MEAN AND STD
# https://www.kaggle.com/kyakovlev/ieee-fe-with-some-eda
def encode_AG(main_columns, uids, aggregations=['mean'], train_df=X_train, test_df=X_test, 
              fillna=True, usena=False):
    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS
    for main_column in main_columns:  
        for col in uids:
            for agg_type in aggregations:
                new_col_name = main_column+'_'+col+'_'+agg_type
                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])
                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan
                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(
                                                        columns={agg_type: new_col_name})

                temp_df.index = list(temp_df[col])
                temp_df = temp_df[new_col_name].to_dict()   

                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')
                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')
                
                if fillna:
                    train_df[new_col_name].fillna(-1,inplace=True)
                    test_df[new_col_name].fillna(-1,inplace=True)
                
                print("'"+new_col_name+"'",', ',end='')
                
# LABEL ENCODE
def encode_LE(col,train=X_train,test=X_test,verbose=True):
    df_comb = pd.concat([train[col],test[col]],axis=0)
    df_comb,_ = df_comb.factorize(sort=True)
    nm = col
    if df_comb.max()>32000: 
        train[nm] = df_comb[:len(train)].astype('int32')
        test[nm] = df_comb[len(train):].astype('int32')
    else:
        train[nm] = df_comb[:len(train)].astype('int16')
        test[nm] = df_comb[len(train):].astype('int16')
    del df_comb; x=gc.collect()
    if verbose: print(nm,', ',end='')
```

We use the function 'encode_CB' to combine columns card1+addr1, card1+addr1+P_emaildomain
```python
encode_CB('card1','addr1')
encode_CB('card1_addr1','P_emaildomain')
```
Use the function 'encode_LE' to get the aggregated mean and std for the feature created above and add it as new features.
```python
encode_AG(['TransactionAmt','D9','D11'],['card1','card1_addr1','card1_addr1_P_emaildomain'],['mean','std'],usena=True)
```
Now let's run XGBoost with the input data containing the newly added features.
```
XGBoost version: 0.90
[0]	validation_0-auc:0.673694
Will train until validation_0-auc hasn't improved in 100 rounds.
[50]	validation_0-auc:0.883513
[100]	validation_0-auc:0.930568
[150]	validation_0-auc:0.965679
[200]	validation_0-auc:0.978922
[250]	validation_0-auc:0.979067
[300]	validation_0-auc:0.977088
Stopping. Best iteration:
[218]	validation_0-auc:0.979635
```
Great! The score has increased from 0.86896 -> 0.979635.

[Lin]: https://arxiv.org/abs/1612.03144
